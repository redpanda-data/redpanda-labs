#!/bin/bash

# =============================================================================
# SPARK DEPLOYMENT SCRIPT WITH DYNAMIC MINIO IP RESOLUTION
# =============================================================================
#
# This script solves a  DNS resolution issue in Kubernetes:
#
# PROBLEM: Iceberg and Spark sometimes generate S3 URLs in bucket-style format
# (bucket.hostname) which don't resolve in Kubernetes DNS. For example:
# - redpanda.iceberg-minio-hl.iceberg-lab.svc.cluster.local
#
# SOLUTION: We dynamically resolve the MinIO Pod IP and create hostAliases
# that map bucket-style hostnames to the actual MinIO Pod IP.
#
# This approach ensures reliable S3 connectivity regardless of how URLs
# are generated by the Iceberg/Spark stack.
#
# =============================================================================

set -euo pipefail

# Configuration constants
NAMESPACE="iceberg-lab"
MINIO_POD_SELECTOR="v1.min.io/tenant=iceberg-minio"
SPARK_TEMPLATE="spark-template.yaml"
SPARK_OUTPUT="spark.yaml"

echo "=== Deploying Spark with Dynamic MinIO IP ==="

# =============================================================================
# VALIDATE MINIO AVAILABILITY
# =============================================================================
# Ensure MinIO is running before attempting to deploy Spark
# This prevents deployment failures due to missing dependencies

if ! kubectl get pods -n "$NAMESPACE" -l "$MINIO_POD_SELECTOR" --no-headers 2>/dev/null | grep -q Running; then
    echo "ERROR: No running MinIO Pods found with selector '$MINIO_POD_SELECTOR' in namespace '$NAMESPACE'"
    echo "Please ensure MinIO is deployed and running before deploying Spark."
    echo ""
    echo "To deploy MinIO, run:"
    echo "  kubectl apply -f minio-tenant-values.yaml"
    echo "  kubectl wait --for=condition=ready pod -l $MINIO_POD_SELECTOR --namespace $NAMESPACE --timeout=300s"
    exit 1
fi

# =============================================================================
# RESOLVE MINIO POD IP
# =============================================================================
# Get the actual IP address of the MinIO pod for hostAliases configuration
# This IP will be used to resolve bucket-style S3 URLs

MINIO_IP=$(kubectl get pod -n "$NAMESPACE" -l "$MINIO_POD_SELECTOR" -o jsonpath='{.items[0].status.podIP}' 2>/dev/null)

if [[ -z "$MINIO_IP" ]]; then
    echo "ERROR: Failed to resolve MinIO pod IP"
    echo "This could indicate:"
    echo "  - MinIO pod is not running"
    echo "  - Network policies blocking access"
    echo "  - Insufficient RBAC permissions"
    echo ""
    echo "Debug steps:"
    echo "  kubectl get pods -n $NAMESPACE -l $MINIO_POD_SELECTOR"
    echo "  kubectl describe pod -n $NAMESPACE -l $MINIO_POD_SELECTOR"
    exit 1
fi

echo "✓ MinIO IP resolved: $MINIO_IP"

# =============================================================================
# GENERATE SPARK DEPLOYMENT CONFIGURATION
# =============================================================================
# Create a deployment configuration with:
# 1. Dynamic IP injection for hostAliases
# 2. Proper resource allocation for Spark workloads
# 3. Health checks for reliable operation
# 4. Environment variables for S3 authentication

cat > "$SPARK_OUTPUT" << EOF
# =============================================================================
# SPARK DEPLOYMENT FOR ICEBERG ANALYTICS
# =============================================================================
# Generated by deploy-spark.sh with dynamic MinIO IP resolution
# This deployment provides a Jupyter notebook environment with Spark and Iceberg
# for querying Redpanda topics as analytical tables

apiVersion: apps/v1
kind: Deployment
metadata:
  name: spark-iceberg
  namespace: iceberg-lab
  labels:
    app: spark-iceberg
    deployment-method: dynamic-script
spec:
  replicas: 1
  selector:
    matchLabels:
      app: spark-iceberg
  template:
    metadata:
      labels:
        app: spark-iceberg
      annotations:
        # Track deployment revisions for rollback capability
        deployment.kubernetes.io/revision: "$(date +%s)"
        # Store MinIO IP for debugging and monitoring
        minio.ip: "$MINIO_IP"
    spec:
      # =============================================================================
      # DNS RESOLUTION FOR S3 BUCKET-STYLE URLS
      # =============================================================================
      # Iceberg and Spark sometimes generate S3 URLs in bucket-style format
      # (bucket.hostname) which don't resolve in Kubernetes DNS
      # These hostAliases map bucket-style names to the MinIO pod IP
      hostAliases:
        - ip: "$MINIO_IP"  # MinIO pod IP (resolved dynamically)
          hostnames:
          # Support bucket-style S3 URLs for Iceberg tables
          # Format: bucket.hostname where bucket="redpanda"
          - "redpanda.iceberg-minio-hl.iceberg-lab.svc.cluster.local"
          # Support namespace-prefixed bucket names
          - "lab.redpanda.iceberg-minio-hl.iceberg-lab.svc.cluster.local"
          # Docker Compose compatibility (for development)
          - "redpanda.minio"

      # =============================================================================
      # NODE SCHEDULING AND RESOURCE ISOLATION
      # =============================================================================
      # Schedule on dedicated Spark worker node for performance isolation
      # Prevents interference from other workloads
      nodeSelector:
        kubernetes.io/hostname: kind-worker

      # Tolerate the dedicated taint on Spark worker node
      # This ensures only Spark workloads run on the designated node
      tolerations:
        - key: dedicated
          operator: Equal
          value: spark
          effect: NoSchedule

      containers:
        - name: spark
          # Custom image with Spark, Iceberg, and Jupyter
          # Built for ARM64 architecture (Apple Silicon compatibility)
          image: spark-iceberg-jupyter:latest
          imagePullPolicy: Never  # Use local image from kind load

          ports:
            - containerPort: 8888  # Jupyter notebook interface
            - containerPort: 10000 # Spark Thrift server (if needed)

          # =============================================================================
          # S3/MINIO AUTHENTICATION
          # =============================================================================
          # AWS SDK environment variables for MinIO authentication
          # Spark's S3FileIO will use these for object storage operations
          env:
            - name: AWS_ACCESS_KEY_ID
              value: minio
            - name: AWS_SECRET_ACCESS_KEY
              value: minio123
            - name: AWS_REGION
              value: local

          # =============================================================================
          # RESOURCE ALLOCATION
          # =============================================================================
          # Resource requests and limits for predictable performance
          # Spark requires substantial memory for in-memory processing
          resources:
            requests:
              memory: "2Gi"   # Minimum memory for Spark driver
              cpu: "1000m"    # 1 CPU core minimum
            limits:
              memory: "4Gi"   # Maximum memory allocation
              cpu: "2000m"    # 2 CPU cores maximum

          # =============================================================================
          # HEALTH CHECKS
          # =============================================================================
          # Liveness probe ensures Jupyter is running and responsive
          # Uses /tree endpoint which loads quickly
          livenessProbe:
            httpGet:
              path: /tree
              port: 8888
            initialDelaySeconds: 60  # Allow time for Spark initialization
            periodSeconds: 30

          # Readiness probe determines when pod is ready to serve traffic
          # More frequent checks for faster load balancer updates
          readinessProbe:
            httpGet:
              path: /tree
              port: 8888
            initialDelaySeconds: 30
            periodSeconds: 10
EOF

echo "✓ Spark configuration generated: $SPARK_OUTPUT"

# =============================================================================
# DEPLOY AND VALIDATE
# =============================================================================
# Apply the configuration and wait for successful deployment

echo "Applying Spark deployment configuration..."
kubectl apply -f "$SPARK_OUTPUT" -n "$NAMESPACE"

# Wait for deployment to be ready with timeout
echo "Waiting for Spark deployment to be ready..."
if kubectl rollout status deployment/spark-iceberg -n "$NAMESPACE" --timeout=300s; then
    echo "✓ Spark deployment successful"
else
    echo "✗ Spark deployment failed or timed out"
    echo "Check logs with: kubectl logs -n $NAMESPACE deployment/spark-iceberg"
    exit 1
fi

# =============================================================================
# DEPLOYMENT SUMMARY AND NEXT STEPS
# =============================================================================

echo ""
echo "=== Deployment Summary ==="
echo "✓ Spark deployment updated successfully"
echo "✓ MinIO IP resolved and configured: $MINIO_IP"
echo "✓ Configuration file: $SPARK_OUTPUT"
echo "✓ hostAliases configured for bucket-style URL resolution"
echo ""
echo "Next steps:"
echo "1. Access Jupyter: kubectl port-forward -n $NAMESPACE deploy/spark-iceberg 8888:8888"
echo "2. Test SQL queries: %%sql SELECT * FROM redpanda.key_value"
echo "3. Re-run this script if MinIO Pods restart or IPs change"
echo ""
echo "For troubleshooting, see: README.adoc"
echo ""
echo "Architecture notes:"
echo "- Uses dynamic IP resolution to solve Kubernetes DNS limitations"
echo "- Supports both path-style and bucket-style S3 URLs"
echo "- Provides dedicated node scheduling for performance isolation"
echo "- Includes comprehensive health checks and resource management"
